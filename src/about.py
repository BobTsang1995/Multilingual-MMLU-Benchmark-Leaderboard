from dataclasses import dataclass
from enum import Enum

@dataclass
class Task:
    benchmark: str
    metric: str
    col_name: str


# Select your tasks here
# ---------------------------------------------------
class Tasks(Enum):
    # task_key in the json file, metric_key in the json file, name to display in the leaderboard 
    task0 = Task("mmmlu", "acc", "MMMLU")
    # task1 = Task("mmlu", "acc", "MMLU")
    # task2 = Task("cmmlu", "acc", "CMMLU")
    task3 = Task("mmmlu_ar", "acc", "MMMLU_AR")
    task4 = Task("mmmlu_bn", "acc", "MMMLU_BN")
    task5 = Task("mmmlu_de", "acc", "MMMLU_DE")
    task6 = Task("mmmlu_es", "acc", "MMMLU_ES")
    task7 = Task("mmmlu_fr", "acc", "MMMLU_FR")
    task8 = Task("mmmlu_hi", "acc", "MMMLU_HI")
    task9 = Task("mmmlu_id", "acc", "MMMLU_ID")
    task10 = Task("mmmlu_it", "acc", "MMMLU_IT")
    task11 = Task("mmmlu_ja", "acc", "MMMLU_JA")
    task12 = Task("mmmlu_ko", "acc", "MMMLU_KO")
    task13 = Task("mmmlu_pt", "acc", "MMMLU_PT")
    task14 = Task("mmmlu_sw", "acc", "MMMLU_SW")
    task15 = Task("mmmlu_yo", "acc", "MMMLU_YO")
    task16 = Task("mmmlu_zh", "acc", "MMMLU_ZH")
NUM_FEWSHOT = 5 # Change with your few shot
# ---------------------------------------------------



# Your leaderboard name
TITLE = """<h1 align="center" id="space-title">Multilingual MMLU Benchmark Leaderboard</h1>"""

# What does your leaderboard evaluate?
INTRODUCTION_TEXT = """
**Multilingual MMLU Benchmark Leaderboard:** This leaderboard is dedicated to evaluating the performance of both open-source and closed-source language models on the Multilingual MMLU benchmark. It assesses their memorization, reasoning, and linguistic capabilities across a wide range of languages. The leaderboard consolidates multiple MMLU datasets, originally created or manually translated into various languages, to provide a comprehensive evaluation of multilingual understanding in LLMs.
"""
INTRODUCTION_TEXT_ZH = """
**å¤šè¯­è¨€ MMLU åŸºå‡†æ¦œå•ï¼š** è¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„è¯„æµ‹æ¦œå•ï¼Œæ—¨åœ¨è¯„ä¼°å¼€æºå’Œé—­æºè¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ MMLU åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œæ¶µç›–è®°å¿†ã€æ¨ç†å’Œè¯­è¨€èƒ½åŠ›ã€‚è¯¥æ¦œå•æ•´åˆäº†å¤šä¸ª MMLU æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æœ€åˆä¸ºå¤šç§è¯­è¨€åˆ›å»ºæˆ–æ‰‹åŠ¨ç¿»è¯‘ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç†è§£ä¸Šçš„èƒ½åŠ›ã€‚
"""
# Which evaluations are you running? how can people reproduce what you have?
# TODO: Update number of benchmarks
LLM_BENCHMARKS_TEXT = """
## ğŸ’¡ About "Multilingual Benchmark MMLU Leaderboard"

- Press release: [TBD - XXX](#), [TBD - XXX](#), [TBD - XXX](#), [TBD - XXX](#)
- YouTube: [TBD - XXX](#)

### Overview
The **Multilingual Massive Multitask Language Understanding (MMMLU)** benchmark is a comprehensive evaluation platform designed to assess the general knowledge capabilities of AI models across a wide range of domains. It includes a series of **Question Answering (QA)** tasks across **57 distinct domains**, ranging from elementary-level knowledge to advanced professional subjects such as law, physics, history, and computer science.

### Translation Effort
For this evaluation, we used the **OpenAI MMMLU dataset**, which has been extensively curated and tested for a multilingual understanding of AI models. The dataset includes 14 different languages and is specifically designed to assess how well AI models can handle a wide range of general knowledge tasks across 57 domains. 

While the translation of the test set was performed by OpenAI, it ensures a high level of accuracy and reliability for evaluating multilingual models. By leveraging this pre-existing, professionally curated dataset, we aim to focus on model performance across multiple languages, without the need for additional translations from our side.

### Commitment to Multilingual AI
By focusing on human-powered translations and publishing both the translated test sets and evaluation code, we aim to promote the development of AI models that can handle multilingual tasks with greater accuracy. This reflects our commitment to improving AIâ€™s performance in underrepresented languages and making technology more inclusive and effective globally.

### Locales Covered
The MMMLU benchmark includes a test set translated into the following locales:
- **AR_XY**: Arabic
- **BN_BD**: Bengali
- **DE_DE**: German
- **ES_LA**: Spanish
- **FR_FR**: French
- **HI_IN**: Hindi
- **ID_ID**: Indonesian
- **IT_IT**: Italian
- **JA_JP**: Japanese
- **KO_KR**: Korean
- **PT_BR**: Brazilian Portuguese
- **SW_KE**: Swahili
- **YO_NG**: Yoruba
- **ZH_CN**: Simplified Chinese

### Purpose
The MMMLU Leaderboard aims to provide a unified benchmark for comparing AI model performance across these multiple languages and diverse domains. With the inclusion of the **QA task** across **57 domains**, it evaluates how well models perform in answering general knowledge questions in multiple languages, ensuring a high standard of multilingual understanding and reasoning.

### Goals
Our primary goal is to provide a reliable comparison for AI models across different languages and domains, helping developers and researchers evaluate and improve their modelsâ€™ multilingual capabilities. By emphasizing high-quality translations and including a broad range of topics, we strive to make AI models more robust and useful across diverse communities worldwide.

### ğŸ¤— How it works

Submit a model for automated evaluation on our clusters on the "Submit here" tab!

### ğŸ“ˆ Tasks

We evaluate models on a variety of key benchmarks, with a focus on **Multilingual Massive Multitask Language Understanding (MMLU)** and its variants, including MMLU, C-MMLU, ArabicMMLU, KoreanMMLU, MalayMMLU, and others. These benchmarks assess general knowledge across a wide range of topics from 57 categories, such as law, physics, history, and computer science.

The evaluation is performed using the [OpenCompass framework](https://github.com/open-compass/opencompass), a unified platform for evaluating language models across multiple tasks. OpenCompass allows us to execute these evaluations efficiently and at scale, covering multiple languages and benchmarks.

For detailed information on the tasks, please refer to the "Tasks" tab in the OpenCompass framework.

Notes:
- The evaluations are all 5-shot.
- The results are normalized with the following formula: `normalized_value = (raw_value - random_baseline) / (max_value - random_baseline)`, where `random_baseline` is `0` for generative tasks and `1/n` for multi-choice QA  with `n` choices.
- Results are aggregated by calculating the average of all the tasks for a given language.

### ğŸ” Results

You can find:

- Detailed numerical results in the [results dataset](link_to_results)
- Community queries and running status in the [requests dataset](link_to_requests)

### âœ… Reproducibility

To reproduce the results, you can use [our fork of lm_eval](#), as not all of our PRs are currently integrated into the main repository.

## ğŸ™Œ Acknowledgements

This leaderboard was developed as part of the [#ProjectName](link_to_project) led by [OrganizationName](link_to_organization) thanks to the donation of high-quality evaluation datasets by:

- [Institution 1](link_to_institution_1)
- [Institution 2](link_to_institution_2)
- [Institution 3](link_to_institution_3)
- [Institution 4](link_to_institution_4)
- [Institution 5](link_to_institution_5)
- [Institution 6](link_to_institution_6)
- [Institution 7](link_to_institution_7)
- [Institution 8](link_to_institution_8)
- [Institution 9](link_to_institution_9)

The entities above are ordered chronologically by the date they joined the project. However, the logos in the footer are ordered by the number of datasets donated.

Thank you in particular to:
- Task implementation: [Name 1], [Name 2], [Name 3], [Name 4], [Name 5], [Name 6], [Name 7], [Name 8], [Name 9], [Name 10]
- Leaderboard implementation: [Name 11], [Name 12]
- Model evaluation: [Name 13], [Name 14], [Name 15], [Name 16], [Name 17]
- Communication: [Name 18], [Name 19]
- Organization & colab leads: [Name 20], [Name 21], [Name 22], [Name 23], [Name 24], [Name 25], [Name 26], [Name 27], [Name 28], [Name 29], [Name 30]

For information about the dataset authors please check the corresponding Dataset Cards (linked in the "Tasks" tab) and papers (included in the "Citation" section below). We would like to specially thank the teams that created or open-sourced their datasets specifically for the leaderboard (in chronological order):
- [Dataset1 Placeholder] and [Dataset2 Placeholder]: [Team members placeholder]
- [Dataset3 Placeholder], [Dataset4 Placeholder] and [Dataset5 Placeholder]: [Team members placeholder]
- [Dataset6 Placeholder]: [Team members placeholder]

We also thank [Institution1 Placeholder], [Institution2 Placeholder], [Organization Placeholder], [Person1 Placeholder], [Person2 Placeholder] and [Institution3 Placeholder] for sponsoring the inference GPUs.

## ğŸš€ Collaborate!

We would like to create a leaderboard as diverse as possible, reach out if you would like us to include your evaluation dataset!

Comments and suggestions are more than welcome! Visit the [ğŸ‘ Community](<Community Page Placeholder>) page, tell us what you think about La Leaderboard and how we can improve it, or go ahead and open a PR!

Thank you very much! ğŸ’›

"""

LLM_BENCHMARKS_TEXT_ZH = """
## ğŸ’¡ å…³äº "å¤šè¯­è¨€åŸºå‡† MMLU æ’è¡Œæ¦œ"

- æ–°é—»ç¨¿ï¼š[å¾…å®š - XXX](#), [å¾…å®š - XXX](#), [å¾…å®š - XXX](#), [å¾…å®š - XXX](#)
- YouTubeï¼š[å¾…å®š - XXX](#)

### æ¦‚è¿°
**å¤šè¯­è¨€å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ (MMMLU)** åŸºå‡†æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼° AI æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„é€šç”¨çŸ¥è¯†èƒ½åŠ›ã€‚å®ƒåŒ…æ‹¬ä¸€ç³»åˆ—è·¨è¶Š **57 ä¸ªä¸åŒé¢†åŸŸ** çš„ **é—®ç­” (QA)** ä»»åŠ¡ï¼Œä»åŸºç¡€çŸ¥è¯†åˆ°æ³•å¾‹ã€ç‰©ç†ã€å†å²ã€è®¡ç®—æœºç§‘å­¦ç­‰é«˜çº§ä¸“ä¸šä¸»é¢˜ã€‚

### ç¿»è¯‘å·¥ä½œ
å¯¹äºæœ¬æ¬¡è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† **OpenAI MMMLU æ•°æ®é›†**ï¼Œè¯¥æ•°æ®é›†å·²ç»å¹¿æ³›ç­–åˆ’å¹¶æµ‹è¯•äº† AI æ¨¡å‹çš„å¤šè¯­è¨€ç†è§£èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬ 14 ç§ä¸åŒçš„è¯­è¨€ï¼Œä¸“é—¨è®¾è®¡ç”¨æ¥è¯„ä¼° AI æ¨¡å‹åœ¨ 57 ä¸ªé¢†åŸŸä¸­å¤„ç†å„ç§é€šç”¨çŸ¥è¯†ä»»åŠ¡çš„èƒ½åŠ›ã€‚

å°½ç®¡æµ‹è¯•é›†çš„ç¿»è¯‘æ˜¯ç”± OpenAI æ‰§è¡Œçš„ï¼Œä½†å®ƒç¡®ä¿äº†è¯„ä¼°å¤šè¯­è¨€æ¨¡å‹çš„é«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é€šè¿‡åˆ©ç”¨è¿™ä¸ªé¢„å…ˆå­˜åœ¨çš„ã€ä¸“ä¸šç­–åˆ’çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸“æ³¨äºæ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸­çš„è¡¨ç°ï¼Œè€Œæ— éœ€æˆ‘ä»¬é¢å¤–è¿›è¡Œç¿»è¯‘å·¥ä½œã€‚

### è‡´åŠ›äºå¤šè¯­è¨€ AI
é€šè¿‡ä¸“æ³¨äºäººå·¥ç¿»è¯‘å¹¶å…¬å¼€ç¿»è¯‘åçš„æµ‹è¯•é›†å’Œè¯„ä¼°ä»£ç ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¿ƒè¿›èƒ½å¤Ÿå¤„ç†å¤šè¯­è¨€ä»»åŠ¡çš„ AI æ¨¡å‹çš„å¼€å‘ï¼Œå¹¶ä½¿å…¶æ›´åŠ å‡†ç¡®ã€‚è¿™ä¹Ÿä½“ç°äº†æˆ‘ä»¬åœ¨æ”¹å–„ AI åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„è¡¨ç°ä»¥åŠæ¨åŠ¨å…¨çƒæŠ€æœ¯åŒ…å®¹æ€§æ–¹é¢çš„æ‰¿è¯ºã€‚

### æ¶µç›–çš„è¯­è¨€åŒºåŸŸ
MMMLU åŸºå‡†åŒ…æ‹¬ä»¥ä¸‹è¯­è¨€åŒºåŸŸçš„ç¿»è¯‘æµ‹è¯•é›†ï¼š
- **AR_XY**ï¼šé˜¿æ‹‰ä¼¯è¯­
- **BN_BD**ï¼šå­ŸåŠ æ‹‰è¯­
- **DE_DE**ï¼šå¾·è¯­
- **ES_LA**ï¼šè¥¿ç­ç‰™è¯­
- **FR_FR**ï¼šæ³•è¯­
- **HI_IN**ï¼šå°åœ°è¯­
- **ID_ID**ï¼šå°å°¼è¯­
- **IT_IT**ï¼šæ„å¤§åˆ©è¯­
- **JA_JP**ï¼šæ—¥è¯­
- **KO_KR**ï¼šéŸ©è¯­
- **PT_BR**ï¼šå·´è¥¿è‘¡è„ç‰™è¯­
- **SW_KE**ï¼šæ–¯ç“¦å¸Œé‡Œè¯­
- **YO_NG**ï¼šçº¦é²å·´è¯­
- **ZH_CN**ï¼šç®€ä½“ä¸­æ–‡

### ç›®çš„
MMMLU æ’è¡Œæ¦œæ—¨åœ¨ä¸ºæ¯”è¾ƒ AI æ¨¡å‹åœ¨è¿™äº›å¤šè¯­è¨€å’Œå¤šé¢†åŸŸä¸­çš„è¡¨ç°æä¾›ç»Ÿä¸€çš„åŸºå‡†ã€‚é€šè¿‡åŒ…æ‹¬ **57 ä¸ªé¢†åŸŸ** ä¸­çš„ **é—®ç­”ä»»åŠ¡**ï¼Œå®ƒè¯„ä¼°äº†æ¨¡å‹åœ¨å¤šè¯­è¨€ä¸­å›ç­”é€šç”¨çŸ¥è¯†é—®é¢˜çš„èƒ½åŠ›ï¼Œç¡®ä¿äº†å¤šè¯­è¨€ç†è§£å’Œæ¨ç†çš„é«˜æ ‡å‡†ã€‚

### ç›®æ ‡
æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯ä¸º AI æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œé¢†åŸŸä¸­çš„è¡¨ç°æä¾›å¯é çš„æ¯”è¾ƒï¼Œå¸®åŠ©å¼€å‘è€…å’Œç ”ç©¶äººå‘˜è¯„ä¼°å’Œæé«˜ä»–ä»¬æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚é€šè¿‡å¼ºè°ƒé«˜è´¨é‡çš„ç¿»è¯‘å’ŒåŒ…æ‹¬å¹¿æ³›çš„ä¸»é¢˜ï¼Œæˆ‘ä»¬åŠªåŠ›ä½¿ AI æ¨¡å‹åœ¨å…¨çƒä¸åŒç¤¾åŒºä¸­æ›´åŠ ç¨³å¥å’Œæœ‰ç”¨ã€‚

### ğŸ¤— å·¥ä½œåŸç†

åœ¨ "æäº¤è¿™é‡Œ" æ ‡ç­¾é¡µä¸Šæäº¤æ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼

### ğŸ“ˆ ä»»åŠ¡

æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨å¤šä¸ªå…³é”®åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨ **å¤šè¯­è¨€å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ (MMLU)** åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬ MMLUã€C-MMLUã€é˜¿æ‹‰ä¼¯è¯­ MMLUã€éŸ©è¯­ MMLUã€é©¬æ¥è¯­ MMLU ç­‰ã€‚ è¿™äº›åŸºå‡†è¯„ä¼°äº†æ¥è‡ª 57 ä¸ªç±»åˆ«ï¼ˆå¦‚æ³•å¾‹ã€ç‰©ç†ã€å†å²å’Œè®¡ç®—æœºç§‘å­¦ç­‰ï¼‰çš„ä¸€èˆ¬çŸ¥è¯†ã€‚

è¯„ä¼°ä½¿ç”¨ [OpenCompass æ¡†æ¶](https://github.com/open-compass/opencompass) æ‰§è¡Œï¼Œè¯¥å¹³å°ç»Ÿä¸€äº†å¯¹è¯­è¨€æ¨¡å‹çš„å¤šä»»åŠ¡è¯„ä¼°ã€‚OpenCompass ä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°ã€å¤§è§„æ¨¡åœ°æ‰§è¡Œè¿™äº›è¯„ä¼°ï¼Œè¦†ç›–å¤šç§è¯­è¨€å’ŒåŸºå‡†ã€‚

æœ‰å…³ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ OpenCompass æ¡†æ¶ä¸­çš„ "ä»»åŠ¡" æ ‡ç­¾é¡µã€‚

æ³¨ï¼š
- æ‰€æœ‰è¯„ä¼°å‡ä¸º 5-shot ä»»åŠ¡ã€‚
- ç»“æœé‡‡ç”¨ä»¥ä¸‹å…¬å¼æ ‡å‡†åŒ–ï¼š`normalized_value = (raw_value - random_baseline) / (max_value - random_baseline)`ï¼Œå…¶ä¸­ `random_baseline` å¯¹äºç”Ÿæˆä»»åŠ¡ä¸º `0`ï¼Œå¯¹äºå¤šé€‰ QA ä¸º `1/n`ï¼ˆ`n` ä¸ºé€‰æ‹©æ•°ï¼‰ã€‚
- ç»“æœé€šè¿‡è®¡ç®—ç»™å®šè¯­è¨€çš„æ‰€æœ‰ä»»åŠ¡çš„å¹³å‡å€¼æ¥æ±‡æ€»ã€‚

### ğŸ” ç»“æœ

ä½ å¯ä»¥æ‰¾åˆ°ï¼š

- è¯¦ç»†çš„æ•°å€¼ç»“æœåœ¨ [ç»“æœæ•°æ®é›†](link_to_results)
- ç¤¾åŒºæŸ¥è¯¢å’Œè¿è¡ŒçŠ¶æ€åœ¨ [è¯·æ±‚æ•°æ®é›†](link_to_requests)

### âœ… å¯å¤ç°æ€§

è¦å¤ç°ç»“æœï¼Œä½ å¯ä»¥ä½¿ç”¨ [æˆ‘ä»¬ fork çš„ lm_eval](#)ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰çš„ PR éƒ½å·²é›†æˆåˆ°ä¸»ä»“åº“ä¸­ã€‚

## ğŸ™Œ è‡´è°¢

è¿™ä¸ªæ’è¡Œæ¦œæ˜¯ [#ProjectName](link_to_project) é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œç”± [OrganizationName](link_to_organization) é¢†å¯¼ï¼Œæ„Ÿè°¢ä»¥ä¸‹æœºæ„æèµ äº†é«˜è´¨é‡çš„è¯„ä¼°æ•°æ®é›†ï¼š

- [æœºæ„ 1](link_to_institution_1)
- [æœºæ„ 2](link_to_institution_2)
- [æœºæ„ 3](link_to_institution_3)
- [æœºæ„ 4](link_to_institution_4)
- [æœºæ„ 5](link_to_institution_5)
- [æœºæ„ 6](link_to_institution_6)
- [æœºæ„ 7](link_to_institution_7)
- [æœºæ„ 8](link_to_institution_8)
- [æœºæ„ 9](link_to_institution_9)

è¿™äº›å®ä½“æŒ‰åŠ å…¥é¡¹ç›®çš„æ—¶é—´é¡ºåºæ’åˆ—ï¼Œç„¶è€Œé¡µè„šçš„ logo æ’åˆ—é¡ºåºæ˜¯æŒ‰ç…§æèµ æ•°æ®é›†çš„æ•°é‡ã€‚

ç‰¹åˆ«æ„Ÿè°¢ï¼š
- ä»»åŠ¡å®ç°ï¼š[å§“å 1]ï¼Œ[å§“å 2]ï¼Œ[å§“å 3]ï¼Œ[å§“å 4]ï¼Œ[å§“å 5]ï¼Œ[å§“å 6]ï¼Œ[å§“å 7]ï¼Œ[å§“å 8]ï¼Œ[å§“å 9]ï¼Œ[å§“å 10]
- æ’è¡Œæ¦œå®ç°ï¼š[å§“å 11]ï¼Œ[å§“å 12]
- æ¨¡å‹è¯„ä¼°ï¼š[å§“å 13]ï¼Œ[å§“å 14]ï¼Œ[å§“å 15]ï¼Œ[å§“å 16]ï¼Œ[å§“å 17]
- æ²Ÿé€šï¼š[å§“å 18]ï¼Œ[å§“å 19]
- ç»„ç»‡ä¸åä½œé¢†å¯¼ï¼š[å§“å 20]ï¼Œ[å§“å 21]ï¼Œ[å§“å 22]ï¼Œ[å§“å 23]ï¼Œ[å§“å 24]ï¼Œ[å§“å 25]ï¼Œ[å§“å 26]ï¼Œ[å§“å 27]ï¼Œ[å§“å 28]ï¼Œ[å§“å 29]ï¼Œ[å§“å 30]

æœ‰å…³æ•°æ®é›†ä½œè€…çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„æ•°æ®é›†å¡ç‰‡ï¼ˆå¯ä»¥åœ¨ "ä»»åŠ¡" æ ‡ç­¾é¡µä¸­æ‰¾åˆ°ï¼‰ä»¥åŠè®ºæ–‡ï¼ˆåœ¨ "å¼•ç”¨" éƒ¨åˆ†æä¾›ï¼‰ã€‚æˆ‘ä»¬ç‰¹åˆ«æ„Ÿè°¢é‚£äº›ä¸ºæ’è¡Œæ¦œä¸“é—¨åˆ›å»ºæˆ–å¼€æºå…¶æ•°æ®é›†çš„å›¢é˜Ÿï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ï¼š
- [æ•°æ®é›†1 å ä½ç¬¦] å’Œ [æ•°æ®é›†2 å ä½ç¬¦]ï¼š [å›¢é˜Ÿæˆå‘˜å ä½ç¬¦]
- [æ•°æ®é›†3 å ä½ç¬¦]ï¼Œ[æ•°æ®é›†4 å ä½ç¬¦] å’Œ [æ•°æ®é›†5 å ä½ç¬¦]ï¼š [å›¢é˜Ÿæˆå‘˜å ä½ç¬¦]
- [æ•°æ®é›†6 å ä½ç¬¦]ï¼š [å›¢é˜Ÿæˆå‘˜å ä½ç¬¦]

æˆ‘ä»¬è¿˜æ„Ÿè°¢ [æœºæ„1 å ä½ç¬¦]ï¼Œ[æœºæ„2 å ä½ç¬¦]ï¼Œ[ç»„ç»‡å ä½ç¬¦]ï¼Œ[äººå‘˜1 å ä½ç¬¦]ï¼Œ[äººå‘˜2 å ä½ç¬¦] å’Œ [æœºæ„3 å ä½ç¬¦] æä¾›æ¨ç† GPU æ”¯æŒã€‚

## ğŸš€ åˆä½œï¼

æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªå°½å¯èƒ½å¤šæ ·åŒ–çš„æ’è¡Œæ¦œï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬å¦‚æœä½ å¸Œæœ›æˆ‘ä»¬å°†ä½ çš„è¯„ä¼°æ•°æ®é›†åŒ…å«åœ¨å†…ï¼

è¯„è®ºå’Œå»ºè®®éå¸¸æ¬¢è¿ï¼è¯·è®¿é—® [ğŸ‘ ç¤¾åŒº](<Community Page Placeholder>) é¡µé¢ï¼Œå‘Šè¯‰æˆ‘ä»¬ä½ å¯¹ La æ’è¡Œæ¦œçš„çœ‹æ³•ä»¥åŠæˆ‘ä»¬å¦‚ä½•æ”¹è¿›ï¼Œæˆ–è€…ç›´æ¥æ‰“å¼€ä¸€ä¸ª PRï¼

éå¸¸æ„Ÿè°¢ï¼ ğŸ’›
"""

EVALUATION_QUEUE_TEXT = """
## Some good practices before submitting a model

### 1) Make sure you can load your model and tokenizer using AutoClasses:
```python
from transformers import AutoConfig, AutoModel, AutoTokenizer
config = AutoConfig.from_pretrained("your model name", revision=revision)
model = AutoModel.from_pretrained("your model name", revision=revision)
tokenizer = AutoTokenizer.from_pretrained("your model name", revision=revision)
```
If this step fails, follow the error messages to debug your model before submitting it. It's likely your model has been improperly uploaded.

Note: make sure your model is public!
Note: if your model needs `use_remote_code=True`, we do not support this option yet but we are working on adding it, stay posted!

### 2) Convert your model weights to [safetensors](https://huggingface.co/docs/safetensors/index)
It's a new format for storing weights which is safer and faster to load and use. It will also allow us to add the number of parameters of your model to the `Extended Viewer`!

### 3) Make sure your model has an open license!
This is a leaderboard for Open LLMs, and we'd love for as many people as possible to know they can use your model ğŸ¤—

### 4) Fill up your model card
When we add extra information about models to the leaderboard, it will be automatically taken from the model card

## In case of model failure
If your model is displayed in the `FAILED` category, its execution stopped.
Make sure you have followed the above steps first.
If everything is done, check you can launch the EleutherAIHarness on your model locally, using the above command without modifications (you can add `--limit` to limit the number of examples per task).
"""

CITATION_BUTTON_LABEL = "Copy the following snippet to cite these results"
CITATION_BUTTON_TEXT = r"""
"""
EVALUATION_QUEUE_TEXT_ZH = """
## æäº¤æ¨¡å‹å‰çš„ä¸€äº›è‰¯å¥½å®è·µ

### 1) ç¡®ä¿ä½ å¯ä»¥ä½¿ç”¨ AutoClasses åŠ è½½ä½ çš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š
```python
from transformers import AutoConfig, AutoModel, AutoTokenizer
config = AutoConfig.from_pretrained("your model name", revision=revision)
model = AutoModel.from_pretrained("your model name", revision=revision)
tokenizer = AutoTokenizer.from_pretrained("your model name", revision=revision)
```
å¦‚æœæ­¤æ­¥éª¤å¤±è´¥ï¼Œè¯·æŒ‰ç…§é”™è¯¯ä¿¡æ¯è¿›è¡Œè°ƒè¯•ï¼Œå¯èƒ½æ˜¯ä½ çš„æ¨¡å‹ä¸Šä¼ ä¸æ­£ç¡®ã€‚

æ³¨æ„ï¼šç¡®ä¿ä½ çš„æ¨¡å‹æ˜¯å…¬å¼€çš„ï¼ æ³¨æ„ï¼šå¦‚æœä½ çš„æ¨¡å‹éœ€è¦ use_remote_code=Trueï¼Œç›®å‰æˆ‘ä»¬ä¸æ”¯æŒè¯¥é€‰é¡¹ï¼Œä½†æˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ·»åŠ æ­¤åŠŸèƒ½ï¼Œè¯·ä¿æŒå…³æ³¨ï¼

2) å°†ä½ çš„æ¨¡å‹æƒé‡è½¬æ¢ä¸º safetensors
è¿™æ˜¯ä¸€ä¸ªæ–°çš„æƒé‡å­˜å‚¨æ ¼å¼ï¼ŒåŠ è½½å’Œä½¿ç”¨æ—¶æ›´å®‰å…¨ã€æ›´å¿«é€Ÿã€‚å®ƒè¿˜å°†å…è®¸æˆ‘ä»¬å°†æ¨¡å‹çš„å‚æ•°æ•°é‡æ·»åŠ åˆ° Extended Viewer ä¸­ï¼

3) ç¡®ä¿ä½ çš„æ¨¡å‹å…·æœ‰å¼€æ”¾è®¸å¯ï¼
è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¼€æ”¾ LLM çš„æ’è¡Œæ¦œï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½å¤šçš„äººçŸ¥é“ä»–ä»¬å¯ä»¥ä½¿ç”¨ä½ çš„æ¨¡å‹ ğŸ¤—

4) å¡«å†™ä½ çš„æ¨¡å‹å¡
å½“æˆ‘ä»¬å°†é¢å¤–çš„ä¿¡æ¯æ·»åŠ åˆ°æ’è¡Œæ¦œæ—¶ï¼Œå®ƒå°†è‡ªåŠ¨ä»æ¨¡å‹å¡ä¸­è·å–ã€‚

æ¨¡å‹å¤±è´¥æ—¶çš„å¤„ç†
å¦‚æœä½ çš„æ¨¡å‹å‡ºç°åœ¨ FAILED åˆ†ç±»ä¸­ï¼Œè¡¨ç¤ºå…¶æ‰§è¡Œåœæ­¢ã€‚ é¦–å…ˆç¡®ä¿ä½ å·²ç»éµå¾ªäº†ä¸Šè¿°æ­¥éª¤ã€‚ å¦‚æœä¸€åˆ‡éƒ½å®Œæˆï¼Œæ£€æŸ¥ä½ æ˜¯å¦å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„å‘½ä»¤åœ¨æœ¬åœ°å¯åŠ¨ EleutherAIHarness æ¥æµ‹è¯•ä½ çš„æ¨¡å‹ï¼ˆä½ å¯ä»¥æ·»åŠ  --limit æ¥é™åˆ¶æ¯ä¸ªä»»åŠ¡çš„ç¤ºä¾‹æ•°ï¼‰ã€‚ """

CITATION_BUTTON_LABEL = "å¤åˆ¶ä»¥ä¸‹ä»£ç å¼•ç”¨è¿™äº›ç»“æœ"
CITATION_BUTTON_TEXT = r"""
"""
LOGOS = [
    "logo/HuggingFace.png",
    "logo/openai-logo.png",
    "logo/logo_qwen.png",
    "logo/CAIS.png"
]